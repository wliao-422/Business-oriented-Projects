{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COS 424 Homework 1: Binary classification of reviews\n",
    "Tom Bertalan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract and introduction\n",
    "Given a dataset of movie, product, and/or restauraunt reviews, each of which includes a boolean positive/negative summary, the I trained several classifiers to predict the summary class given only the review text. The data was divided into 2,400 training samples and 600 test samples. Multiple featurizations and classifiers were tried, with some small variation in generalization error.\n",
    "\n",
    "This PDF and the full code for generating it are both available at [github.com/tsbertalan-homework/424hw1](https://github.com/tsbertalan-homework/424hw1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "hide_input": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (preprocessSentences.py, line 91)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/Users/wenyingliao/Dropbox/Liao_NewFilingSystem_20161115/Spring2017/MachineLearning/Homeworks/HW1/Bartalan_Liao/424hw1/preprocessSentences.py\"\u001b[0;36m, line \u001b[0;32m91\u001b[0m\n\u001b[0;31m    if verbose: print \"Vocab length:\", len(keepset)\u001b[0m\n\u001b[0m                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import preprocessSentences\n",
    "import numpy as np\n",
    "np.random.seed(4)\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "bigFig = (12, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "def thumb(badGood, unicode_symbols=True):\n",
    "    symbols = ['{Disliked}', '{   Liked}']\n",
    "    return symbols[badGood]\n",
    "\n",
    "def wrap(txt, cols=69, indent=11):\n",
    "    assert cols > indent\n",
    "    if '\\n' in txt:\n",
    "        lines = txt.split('\\n')\n",
    "        output = []\n",
    "        for line in lines:\n",
    "            output.append(wrap(line, cols=cols, indent=indent))\n",
    "        return '\\n'.join(output)\n",
    "    else:\n",
    "        if len(txt) > cols:\n",
    "            return txt[:cols] + '\\n' + wrap(' '*indent + txt[cols:].strip(), cols=cols, indent=indent)\n",
    "        else:\n",
    "            return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "def findClosest(vec, val):\n",
    "    \"\"\"Return the index where vec[index] is closest to val.\n",
    "    >>> findClosest([2, 8, 3, 6], 5)\n",
    "    3\n",
    "    \"\"\"\n",
    "    distances = np.abs([val - x for x in vec])\n",
    "    return distances.tolist().index(np.min(distances))\n",
    "\n",
    "def _cachedResult(callback):\n",
    "    '''Decorator to check for cached result.\n",
    "    \n",
    "    Some computations we might not want to do multiple times per RHS\n",
    "    computation. Instead, we cache these until they're explictly invalidated\n",
    "    (at the start of each RHS call).\n",
    "    \n",
    "    Use like\n",
    "    >>> class SomeRHS(RHS):\n",
    "    ...     param1 = 3 \n",
    "    ...     param2 = 4\n",
    "    ...     @_cachedResult\n",
    "    ...     def computedValue(self):\n",
    "    ...         print 'value = %s + %s' % (self.param1, self.param2)\n",
    "    ...         return self.param1 + self.param2\n",
    "    \n",
    "    If we then instantiatem\n",
    "    >>> r = SomeRHS()\n",
    "    \n",
    "    the property will execute the first time\n",
    "    >>> r.computedValue()\n",
    "    value = 3 + 4\n",
    "    7\n",
    "    \n",
    "    but not the second.\n",
    "    >>> r.computedValue()\n",
    "    7\n",
    "    \n",
    "    Note that None is used for cache invalidation, so your methods cannot\n",
    "    return None (or, they can, and will function correctly, but they won't get\n",
    "    the benefit of any caching, since we'll assume the cache is empty each time).\n",
    "    \n",
    "    This also works with properties, but the @property decorator must come first.\n",
    "    >>> class SomeRHS(RHS):\n",
    "    ...     param1 = 3 \n",
    "    ...     param2 = 4\n",
    "    ...     @property\n",
    "    ...     @_cachedResult\n",
    "    ...     def computedValue(self):\n",
    "    ...         print 'value = %s + %s' % (self.param1, self.param2)\n",
    "    ...         return self.param1 + self.param2\n",
    "    >>> r = SomeRHS()\n",
    "    >>> r.computedValue\n",
    "    value = 3 + 4\n",
    "    7\n",
    "    >>> r.computedValue\n",
    "    7\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    propertyName = callback.__name__\n",
    "    def decoratedCallback(self, *args, **kwargs):\n",
    "    \n",
    "        if not hasattr(self, '_caches'):\n",
    "            self._caches = {}\n",
    "        # If there's a cached value, return it. If not, use the callback to\n",
    "        # compute it, cache that result, and return it.\n",
    "        if propertyName in self._caches:\n",
    "            cachedValue = self._caches[propertyName]\n",
    "        else:\n",
    "            cachedValue = self._caches[propertyName] = None\n",
    "        if cachedValue is None:\n",
    "            cachedValue = callback(self, *args, **kwargs)\n",
    "            self._caches[propertyName] = cachedValue\n",
    "        \n",
    "        # However obtained, return the (now cached) value.\n",
    "        return cachedValue\n",
    "    return decoratedCallback\n",
    "\n",
    "\n",
    "class ListTable(list):\n",
    "    \"\"\" Overridden list class which takes a 2-dimensional list of \n",
    "        the form [[1,2,3],[4,5,6]], and renders an HTML Table in \n",
    "        IPython Notebook.\n",
    "        \n",
    "        http://calebmadrigal.com/display-list-as-table-in-ipython-notebook/\"\"\"\n",
    "    \n",
    "    def _repr_html_(self):\n",
    "        html = [\"<table>\"]\n",
    "        for row in self:\n",
    "            html.append(\"<tr>\")\n",
    "            \n",
    "            for col in row:\n",
    "                html.append(\"<td>{0}</td>\".format(col))\n",
    "            \n",
    "            html.append(\"</tr>\")\n",
    "        html.append(\"</table>\")\n",
    "        return ''.join(html)\n",
    "    \n",
    "def getDefaultParams(obj):\n",
    "    '''Find out what the default parameters would have been.'''\n",
    "    defaultVersion = type(obj)()\n",
    "    return defaultVersion.get_params()\n",
    "\n",
    "def getNondefaultParams(obj):\n",
    "    '''Find out what parameters have been changed from the defaults.'''\n",
    "    out = {}\n",
    "    if hasattr(obj, 'get_params'):\n",
    "        defaults = getDefaultParams(obj)\n",
    "        params = obj.get_params()\n",
    "        for key in params:\n",
    "            if params[key] != defaults[key]:\n",
    "                out[key] = params[key]\n",
    "    return out\n",
    "\n",
    "def describe(obj):\n",
    "    params = ', '.join(['%s=%s' % (k, v)\n",
    "                   for (k,v) in getNondefaultParams(obj).items()])\n",
    "    out = obj.__class__.__name__.replace('__main__.', '').replace('Classifier', '')\n",
    "    if len(params) > 0:\n",
    "        out += '(%s)' % (params,)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defaults: logistic regression classifier and bag-of-words featurization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "\n",
    "class ClassifierBase(object):\n",
    "    \n",
    "    def getErrRate(self, X, Y):\n",
    "        predictions = self.predict(X)\n",
    "        numCorrect = (predictions == Y).astype(float).sum()\n",
    "        return 1 - numCorrect / len(Y)\n",
    "    \n",
    "    def getNondefaultParams(self):\n",
    "        return getNondefaultParams(self)\n",
    "    \n",
    "    def sayErr(self, datasetName=None, test=True):\n",
    "        if test:\n",
    "            X = self.test.X\n",
    "            Y = self.test.Y\n",
    "            if datasetName is None: datasetName = 'Testing '\n",
    "        else:\n",
    "            X = self.train.X\n",
    "            Y = self.train.Y\n",
    "            if datasetName is None: datasetName = 'Training'\n",
    "        print '%s error rate is %.2f%%.' % (datasetName, self.getErrRate(X, Y) * 100)\n",
    "        \n",
    "    def __str__(self):\n",
    "        return describe(self)\n",
    "    \n",
    "    def __init__(self, trainingData=None, testingData=None, **kwargs):\n",
    "        if trainingData is None: trainingData = train\n",
    "        if testingData is None: testingData = test\n",
    "        self.train = trainingData\n",
    "        self.test = testingData\n",
    "        super(ClassifierBase, self).__init__(**kwargs)\n",
    "\n",
    "\n",
    "    @property\n",
    "    def otherParent(self):\n",
    "        import inspect\n",
    "        mro = inspect.getmro(type(self))\n",
    "        i = mro.index(ClassifierBase)\n",
    "        return mro[i+1]\n",
    "        \n",
    "    def fit(self, X=None, Y=None, **kwargs):\n",
    "        if X is None: X = self.train.X\n",
    "        if Y is None: Y = self.train.Y\n",
    "        return self.otherParent.fit(self, X, Y, **kwargs)\n",
    "        \n",
    "    def predict(self, X=None, **kwargs):\n",
    "        if X is None: X = self.test.X\n",
    "        return self.otherParent.predict(self, X, **kwargs)\n",
    "        \n",
    "    def _unpackFigax(self, figax, **defaultkwargs):\n",
    "        if figax is None:\n",
    "            fig, ax = plt.subplots(figsize=bigFig, **defaultkwargs)\n",
    "        else:\n",
    "            fig, ax = figax\n",
    "        return fig, ax\n",
    "        \n",
    "    def confusionMatrix(self, figax=None):\n",
    "        fig, ax = self._unpackFigax(figax)\n",
    "        \n",
    "        confusion = sklearn.metrics.confusion_matrix(self.test.Y, self.predict(self.test.X))\n",
    "        \n",
    "        im = ax.imshow(confusion, interpolation='nearest', cmap=plt.get_cmap('Blues'))\n",
    "        fig.colorbar(im)\n",
    "        ax.set_xlabel('predicted')\n",
    "        ax.set_ylabel('true')\n",
    "\n",
    "        ax.set_title('Confusion Matrix for %s' % self)\n",
    "\n",
    "        classnums = [0, 1]\n",
    "        classnames = ['dislike', 'like']\n",
    "        ax.set_xticks(classnums)\n",
    "        ax.set_xticklabels(classnames)\n",
    "        ax.set_yticks(classnums)\n",
    "        ax.set_yticklabels(classnames);\n",
    "        \n",
    "        # Ensure that #samples == tp + tn + fp + fn\n",
    "        assert sum([metrics.truePositives, metrics.trueNegatives,\n",
    "                    metrics.falsePositives, metrics.falseNegatives]) == len(self.test.Y)\n",
    "        \n",
    "        return fig, ax\n",
    "    \n",
    "    @property\n",
    "    @_cachedResult\n",
    "    def predictions(self):\n",
    "        return self.predict(self.test.X)\n",
    "    \n",
    "    @property\n",
    "    @_cachedResult\n",
    "    def accuracysklearn(self):\n",
    "        if hasattr(self, 'score'):\n",
    "            thing = self\n",
    "        else:\n",
    "            assert hasattr(self, 'estimator')\n",
    "            thing = self.estimator\n",
    "        return thing.score(self.test.X, self.test.Y)\n",
    "    \n",
    "    @property\n",
    "    @_cachedResult\n",
    "    def truePositives(self):\n",
    "        return sum(np.logical_and(\n",
    "                self.predictions == self.test.Y,\n",
    "                self.test.Y\n",
    "                                 ).astype(int))\n",
    "    \n",
    "    @property\n",
    "    @_cachedResult\n",
    "    def trueNegatives(self):\n",
    "        return sum(np.logical_and(\n",
    "                self.predictions == self.test.Y,\n",
    "                np.logical_not(self.test.Y)\n",
    "                                 ).astype(int))\n",
    "    \n",
    "    @property\n",
    "    @_cachedResult\n",
    "    def falsePositives(self):\n",
    "        num = sum(self.predictions.astype(int))\n",
    "        out = num - self.truePositives\n",
    "        assert out > 0\n",
    "        return out\n",
    "    \n",
    "    @property\n",
    "    @_cachedResult\n",
    "    def falseNegatives(self):\n",
    "        num = sum(np.logical_not(self.predictions).astype(int))\n",
    "        out = num - self.trueNegatives\n",
    "        assert out > 0\n",
    "        return out\n",
    "    \n",
    "    @property\n",
    "    @_cachedResult\n",
    "    def accuracy(self):\n",
    "        return float(self.truePositives + self.trueNegatives) / len(self.test.Y)\n",
    "    \n",
    "    @property\n",
    "    @_cachedResult\n",
    "    def precision(self):\n",
    "        return float(self.truePositives) / (self.truePositives + self.falsePositives)\n",
    "    \n",
    "    @property\n",
    "    @_cachedResult\n",
    "    def recall(self):\n",
    "        return float(self.truePositives) / (self.truePositives + self.falseNegatives)\n",
    "    \n",
    "    @property\n",
    "    @_cachedResult\n",
    "    def specificity(self):\n",
    "        return float(self.trueNegatives) / (self.trueNegatives + self.falsePositives)\n",
    "    \n",
    "    @property\n",
    "    @_cachedResult\n",
    "    def f1(self):\n",
    "        return 2.0 * self.precision * self.recall / (self.precision + self.recall)\n",
    "    \n",
    "    @property\n",
    "    @_cachedResult\n",
    "    def auc(self):\n",
    "        return sklearn.metrics.roc_auc_score(test.Y, self.predictions)\n",
    "    \n",
    "    @property\n",
    "    @_cachedResult\n",
    "    def scores(self):\n",
    "        if hasattr(self, 'decision_function'):\n",
    "            out = self.decision_function(test.X)\n",
    "        elif hasattr(self, 'predict_proba'):\n",
    "            out = self.predict_proba(test.X)\n",
    "        else:\n",
    "            out = self.estimator.predict_proba(test.X)\n",
    "        if isinstance(out, np.ndarray) and len(out.shape) == 2 and out.shape[1] == 2:\n",
    "            # sklearn's naive Bayes returns both class probabilities.\n",
    "            out = out[:, 1]\n",
    "        return out\n",
    "    \n",
    "    def rocAucPlot(self, figax=None, doctor=True):\n",
    "        fig, ax = self._unpackFigax(figax)\n",
    "        \n",
    "        auc = self.auc\n",
    "        scores = self.scores\n",
    "\n",
    "        # Compute metrics.\n",
    "        fpr, tpr, thresholds = sklearn.metrics.roc_curve(test.Y, scores, pos_label=1)\n",
    "        accuracy = self.accuracy\n",
    "        \n",
    "        # Plot the ROC curve.\n",
    "        plot, = ax.plot(fpr, tpr, label='AUC=%.2f (acc %.1f%%) for %s' % (auc,\n",
    "                                                                         accuracy*100,\n",
    "                                                                         self,\n",
    "                                                                        ))\n",
    "        \n",
    "        # Place a marker at the default classification error.\n",
    "        color = plot.get_color()\n",
    "        iclose = findClosest(tpr, accuracy)\n",
    "        ax.plot(fpr[iclose], tpr[iclose], '+',\n",
    "                picker=5, lw=10, \n",
    "                markersize=32,\n",
    "                markeredgewidth=4,\n",
    "                color=color\n",
    "               )\n",
    "#         ax.scatter(, marker='+', s=1024,\n",
    "#                    edgecolors=color, markeredgecolor=color, facecolors='none')\n",
    "       \n",
    "        \n",
    "        if doctor:\n",
    "            ax.plot(fpr, fpr, linestyle='--')\n",
    "            ax.set_xlabel('False Positive Rate')\n",
    "            ax.set_ylabel('True Positive Rate')\n",
    "            ax.legend(loc='best')\n",
    "            ax.set_xlim(0, 1)\n",
    "            ax.set_ylim(0, 1)\n",
    "            ax.set_title('ROC curve(s) for %s features' % self.train)\n",
    "            \n",
    "        return fig, ax\n",
    "    \n",
    "    def refit(self):\n",
    "        ''''''\n",
    "        self._caches = {}\n",
    "        self.fit(self.train.X, self.train.Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ClassifierGroup(object):\n",
    "    \n",
    "    def __init__(self, classifierList):\n",
    "        self.classifierList = classifierList\n",
    "        \n",
    "    def summaryTable(self):\n",
    "\n",
    "        table = ListTable()\n",
    "        headers = '', 'accuracy', 'accuracysklearn', 'precision', 'recall', 'specificity', 'f1', 'auc'\n",
    "        bold = lambda s: '<b>%s</b>' % s\n",
    "        table.append([bold(header) for header in headers])\n",
    "        for classifier in self.classifierList:\n",
    "            items = [bold(str(classifier))]\n",
    "            items.extend(['%.2f' % getattr(classifier, header)\n",
    "                          for header in headers\n",
    "                          if len(header) > 0])\n",
    "            table.append(items)\n",
    "        return table\n",
    "    \n",
    "    def rocComparison(self):\n",
    "        fig, ax = plt.subplots(figsize=bigFig)\n",
    "        for classifier in self.classifierList:\n",
    "            if classifier is self.classifierList[-1]:\n",
    "                classifier.rocAucPlot(figax=(fig,ax), doctor=True)\n",
    "            else:\n",
    "                classifier.rocAucPlot(figax=(fig,ax))\n",
    "        return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hide_input": true
   },
   "outputs": [],
   "source": [
    " class Data(object):\n",
    "    \n",
    "    def __init__(self, fname, vocab=None):\n",
    "        \n",
    "        # Separate documents and maybe generate vocabulary.\n",
    "        if vocab is None:\n",
    "            word_count_threshold = 5\n",
    "            docs, classes, unused_samples, words = preprocessSentences.tokenize_corpus(fname, train=True)\n",
    "            vocab = preprocessSentences.wordcount_filter(words, num=word_count_threshold)\n",
    "        else:\n",
    "            word_count_threshold = 0\n",
    "            docs, classes, unused_samples = preprocessSentences.tokenize_corpus(fname, train=False)\n",
    "        self.vocab = vocab\n",
    "        self.tokenizedLists = docs\n",
    "        self.tokenizedTexts = [' '.join(tokenlist) for tokenlist in docs]\n",
    "        self.classes = self.Y = np.array([int(c) for c in classes])\n",
    "            \n",
    "        # Generate Bag of Words representations.\n",
    "        self.bows = preprocessSentences.find_wordcounts(docs, vocab)\n",
    "        self.X = np.vstack(bow for bow in self.bows)\n",
    "\n",
    "        # Read in actual texts for inspection.\n",
    "        self.texts = []\n",
    "        with open(fname) as trainfile:\n",
    "            for l in trainfile.readlines():\n",
    "                l = l.strip()\n",
    "                idNum = l.split()[0]\n",
    "                text = l[len(idNum):-1].strip()\n",
    "                self.texts.append(text)\n",
    "        self.representationType = 'default bag of words'\n",
    "        \n",
    "    def applyFeaturizer(self, featurizer, name=None):\n",
    "        self.X = featurizer.transform(self.texts)\n",
    "        if hasattr(self.X, 'todense'):\n",
    "            self.X = self.X.todense()\n",
    "        if name is None:\n",
    "            name = describe(featurizer)\n",
    "        self.representationType = name\n",
    "        \n",
    "    def __str__(self):\n",
    "        return self.representationType\n",
    "    \n",
    "    def item2str(self, index):\n",
    "        return '%s %s' % (thumb(self.Y[index]), self.texts[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I defined a `Data` class which holds the original strings, sentiments (classes), and the default bag-of-words featurization for both the train and test datasets.\n",
    "With this, I loaded the two data sets, and used the provided code to generate a tokenization and bag-of-words featurizations. Note that I reused the vocabulary generated for the training data when featurizing the test data, and so both datasets had 541 features per entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = Data('train.txt')\n",
    "test  = Data('test.txt', vocab=train.vocab)\n",
    "print train.X.shape, test.X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifiers = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an initial exploratory step, I checked the largest number of times someone used the same word in a review,\n",
    "`train.X.max()` and `test.X.max()`. These values were surprisingly high. For sanity, I printed the corresponding raw texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "for data in train, test:\n",
    "    argmax = np.argmax(data.X)\n",
    "    index = int(argmax / data.X.shape[1])\n",
    "    max(data.X[index])\n",
    "    print wrap(data.item2str(index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, while \"suck\" is, per [1], a good indicator of a negative review, I don't think we can definitively say that \"steak\" is a good indicator of a positive review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For classification, I took as my base case the default bag-of-words featurization coupled with `sklearn`'s logistic regression classifier. Note that the `ClassifierBase` mixin just defines some convenience functions like `sayErr`. I trained this with an $L_1$ regularization on the training set. However, throughout this homework, my measure of progress or success was the testing error rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "class LogisticClassifier(ClassifierBase, LogisticRegressionCV): pass\n",
    "\n",
    "logisticClassifier = LogisticClassifier(penalty='l1', solver='liblinear')\n",
    "logisticClassifier.fit()\n",
    "logisticClassifier.sayErr(test=False)\n",
    "logisticClassifier.sayErr()\n",
    "classifiers.append(logisticClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression, despite its name, is a classification method. While multiclass variants exist, in its simplest form, it's appropriate for classifying data divided into two classes, as in this case. The probability of class membership is modeled as a Bernoulli variable whose single parameter is written as a sigmoidal function of function $f$ the features $\\vec x$ [6].\n",
    "$$p(y|\\vec x, \\vec w) = \\mathrm{Ber}(y|\\mathrm{sigm}(f(\\vec x; \\vec w))).$$\n",
    "\n",
    "Usually $f$ is taken to be linear.\n",
    "$$f(\\vec x; \\vec w) = \\vec w^T \\vec x.$$\n",
    "\n",
    "Hence, \"training\" for this model consists of finding a vector of weights $\\vec w$ (and, in fact, an additional parameter $c$) that minimize some loss function $l(\\vec w, c)$. Here, we minimize classification error on the training set, with an $L_1$ regularization to encourage sparsity of the trained $\\vec w$ vector [7].\n",
    "$$\\vec w^* = \\min_{\\vec w, c} l(\\vec w, c).$$\n",
    "$$l(\\vec w, c)\n",
    "=\n",
    "||\\vec w||_1\n",
    "+\n",
    "C \\sum_{i=1}^{2400} \\log(\\exp(-y_i(\\vec x_i^t \\vec w + c)) + 1).\n",
    "$$\n",
    "`sklearn`'s `LogisticRegressionCV` class automatically performs cross-validation to find the optimal value of the parameter $C$, whose value governs the weight given to the regularization relative to the misclassification loss.\n",
    "Using an $L_1$ regularization performs feature selection/sparsification, allowing for the high-dimensional feature vectors seen below.\n",
    "\n",
    "Minimization of the loss is performed using a coordinate-descent algorithm found \n",
    "in the C++ LIBLINEAR library. Alternate gradient descent and Newton conjugate-gradient solvers are also available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to improve the classification?\n",
    "For inspriation, I examined some of the failing cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "predictions = logisticClassifier.predict(test.X)\n",
    "truths = test.Y\n",
    "texts = test.texts\n",
    "i = 0\n",
    "for j in range(len(test.Y)):\n",
    "    if i > 20:\n",
    "        break\n",
    "    if truths[j] != predictions[j]:\n",
    "        i += 1\n",
    "        print wrap(test.item2str(j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of these seem like they might be fixed by considering bigrams--e.g. \"don't like\", \"Not good\", or \"low quality\". Others seem like they'd be harder to fix, like \"...I bought this after I bought a cheapy...\", where they're no longer describing the actual product, but a worse competitor (what [1] calls the \"thwarted expectations\" narrative). For others, like \"Kind of flops around.\", I have to wonder whether that was really the full review, or whether the data cleaner was just overzealous. However, I tried next to work with n-grams and other alternate featurizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternate featurizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-grams (word or character)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since upgrading to 2-grams and beyond gave a hugely increased maximum possible number of features, I examined how results change as I steadily increased the number of features allowed. `sklearn` allows us to choose a cut-off s.t. only the most commonly seen $n$-grams are included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "getFeaturizer = lambda max_features: sklearn.feature_extraction.text.CountVectorizer(\n",
    "        input='content',\n",
    "        analyzer='word',\n",
    "        stop_words='english',\n",
    "\n",
    "        # What sort of n-grams do we want to consider?\n",
    "        ngram_range=(1, 2),\n",
    "\n",
    "        # Exclude uncommon n-grams.\n",
    "        #min_df=.0005,\n",
    "        # Alternately, choose the number of features directly.\n",
    "        max_features=max_features,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hide_input": true,
    "hide_output": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook  # Jupyter progressbar\n",
    "max_features_values = np.logspace(1, np.log10(1000), 50).astype(int)\n",
    "# Shuffle so runtime prediction is expected to be correct.\n",
    "np.random.shuffle(max_features_values)\n",
    "testErrs = []\n",
    "\n",
    "for max_features in tqdm_notebook(max_features_values):\n",
    "    featurizer = getFeaturizer(max_features)\n",
    "    # Set vocabulary.\n",
    "    featurizer.fit(train.texts)\n",
    "    # Find features.\n",
    "    train.applyFeaturizer(featurizer)\n",
    "    test.applyFeaturizer(featurizer)\n",
    "    # Classify.\n",
    "    logisticClassifier = LogisticClassifier(penalty='l1', solver='liblinear')\n",
    "    logisticClassifier.fit()\n",
    "    # Test.\n",
    "    testErr = logisticClassifier.getErrRate(test.X, test.Y) * 100\n",
    "    testErrs.append(testErr)\n",
    "order = np.argsort(max_features_values)\n",
    "max_features_values = max_features_values[order]\n",
    "testErrs = np.array(testErrs)[order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(max_features_values, testErrs)\n",
    "ax.set_xlabel('max_features')\n",
    "ax.set_ylabel('test error [%]')\n",
    "ax.set_xscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, excluding rare bigrams via cut-off doesn't really help. I still somehow did worse than with bag-of-words/unigrams. Anyway, the $L_1$ regularization should be taking care of pruning features,\n",
    "so error generally shouldn't get worse when using a larger vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to word $n$-grams, I tried letter $n$-grams with word boundaries. This should allow different (mis)pellings of the same word to be treated as same instances, if they have common subsequences of $n$ or fewer characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = 6\n",
    "featurizer = sklearn.feature_extraction.text.CountVectorizer(\n",
    "    input='content',\n",
    "    analyzer='char_wb',\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 6),\n",
    "    max_features=2000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "# Set vocabulary\n",
    "featurizer.fit(train.texts)\n",
    "\n",
    "# Find features.\n",
    "train.applyFeaturizer(featurizer)\n",
    "test.applyFeaturizer(featurizer)\n",
    "\n",
    "logisticClassifier = LogisticClassifier(penalty='l1', solver='liblinear')\n",
    "logisticClassifier.fit()\n",
    "logisticClassifier.sayErr(datasetName='Character %d-grams (within word-boundaries) test' % n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Character n-grams can account for spelling variations, but still didn't seem to be much better than word unigrams, and were much slower, since they only makes sense with slightly larger values of n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF reweighting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an alternative method for reconsidering the influence of rare bigrams, I used term a frequency-inverse document frequency featurization to downweight terms which are common in the whole corpus. `skleran` has a featurizer that combines this with the previously-used `CountVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "featurizer = TfidfVectorizer(\n",
    "    max_features=2000,\n",
    "    analyzer='word',\n",
    "    ngram_range=(1, 2),\n",
    "    stop_words='english',\n",
    ")\n",
    "featurizer.fit(train.texts)\n",
    "train.applyFeaturizer(featurizer)\n",
    "test.applyFeaturizer(featurizer)\n",
    "\n",
    "logisticClassifier = LogisticClassifier(penalty='l1', solver='liblinear')\n",
    "logisticClassifier.fit()\n",
    "logisticClassifier.sayErr('TF-IDF test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazingly, performance got *better* when I left stop words in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "featurizer = TfidfVectorizer(\n",
    "    max_features=2000,\n",
    "    analyzer='word',\n",
    "    ngram_range=(1, 2),\n",
    "    stop_words=None,\n",
    ")\n",
    "featurizer.fit(train.texts)\n",
    "train.applyFeaturizer(featurizer)\n",
    "test.applyFeaturizer(featurizer)\n",
    "\n",
    "logisticClassifier = LogisticClassifier(penalty='l1', solver='liblinear')\n",
    "logisticClassifier.fit()\n",
    "logisticClassifier.sayErr('TF-IDF test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I plotted the features directly as an image, after performing a log trasnformation to bring out detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "X = np.array(train.X)\n",
    "Z = np.log(X)\n",
    "Z[Z==-np.inf] = -4\n",
    "fig, ax = plt.subplots(figsize=bigFig)\n",
    "im = ax.imshow(Z, aspect='auto')\n",
    "ax.set_xlabel('feature indices')\n",
    "ax.set_ylabel('data indices')\n",
    "fig.colorbar(im, label='log(features), or %d' % Z.min());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the top few n-grams in the TF-IDF vocabulary ordered by \"prevalence\", which we'll define as the count sum of TF-IDF scores for that feature across all training data samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "items = [(k, v) for (k, v) in featurizer.vocabulary_.items()]\n",
    "grams = [item[0] for item in items]\n",
    "ids = [item[1] for item in items]\n",
    "prevalences = [X[:, j].sum() for j in ids]\n",
    "order = np.argsort(prevalences)[::-1]\n",
    "for i in range(42):\n",
    "    j = order[i]\n",
    "    print 'prevalence %d: \"%s\"' % (prevalences[j], grams[j],)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this is not a list of the most useful words for classifying (as might be found at the top of a decision tree), some of these definitely do look like strong predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternate classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing the TF-IDF bigram featurization, I proceeded to try several different classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first method that the provided referenes suggested was Naive Bayes. To use this, we should examine the class-conditional probabilities of the features, and choose an implementation accordingly. So, I tried to examine the distribution of my TF-IDF features through direct plotting and through histograms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also looked at the bounds of my features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "X = np.array(train.X).ravel()\n",
    "print 'min=%.3f, nonzero_min=%.3f, max=%.3f' % (X.min(), X[X!=0].min(), X.max())\n",
    "print '%.2f%% of values are nonzero'  % (float(X[X!=0].size) / X.size * 100, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the features were set to zero on most items. Values were bounded in $[0, 1]$. In order to say whether each feature behaved more like a categorical or continous variable, I looked for unique values. If I found that a feature actually only took one of a small set of values, then I could argue that it was actually behaviong more like a discrete variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "X = np.array(train.X).ravel()\n",
    "X = X[X!=0]\n",
    "unique = len(set(X))\n",
    "print '%d unique levels (across all features) out of %d nonzero values.' % (unique, X.size)\n",
    "\n",
    "X = np.array(train.X)\n",
    "nNonzero = np.array([sum(feat!=0)\n",
    "                for feat in X.T\n",
    "                ])\n",
    "nlevels = np.array([len(set(feat))\n",
    "                for feat in X.T\n",
    "                ])\n",
    "order = np.argsort(nlevels)\n",
    "nlevels = nlevels[order]\n",
    "nNonzero = nNonzero[order]\n",
    "print np.array(nlevels), 'levels for each of the\\n', len(nlevels), 'features.'\n",
    "print np.array(nNonzero), 'nonzero values for each of the\\n', len(nlevels), 'features.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By this experiment, it certainly seemed like they should be modeled with some sort of continuous distribution. Perhaps something like a very fine-binned histogram (equivalent to rounding all features to e.g. the nearest 1/1000) would reveal more structure, but I'm not going to do that now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I looked at some histograms of the features. First, I looked at the histogram over all nonzero feature values. It looked a lot like some sort of beta distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.hist(X[X!=0], bins=64);\n",
    "ax.set_xlabel('feature value')\n",
    "ax.set_ylabel('unnormalized histogram')\n",
    "ax.set_title('Distribution of nonzero coordinate values, marginal over all cases and classes.');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I tried to examine the class-conditional histograms. I sorted features by their count of nonzero values, and then did histograms of the nonzero values of the top few.\n",
    "Though doubly conditioning on feature and class meant that there wasn't much data to work with, in aggregate, it still seems like individual feature histgorams are roughly beta-shaped. Interestingly, it seemd that the distributions conditioned on the positive class were more sharply peaked than those conditioned on the negative class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "# Find the most active features.\n",
    "X = np.array(train.X)\n",
    "order = np.argsort(X.sum(0))[::-1]\n",
    "Xs = X[:, order]\n",
    "\n",
    "positives = train.Y\n",
    "negatives = np.logical_not(train.Y)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=bigFig)\n",
    "bins = 4\n",
    "nfeatures = 256\n",
    "for i in range(nfeatures):\n",
    "    items = [trio for trio in zip((positives, negatives),\n",
    "                                  ('red', 'black'),\n",
    "                                  (thumb(1), thumb(0)),\n",
    "                                 )]\n",
    "    # Randomly plot red or black first.\n",
    "    np.random.shuffle(items)\n",
    "    for selectors, color, label in items:\n",
    "        x = Xs[selectors, i]\n",
    "        x = x[x!=0]\n",
    "        if i == 0:\n",
    "            ax.hist(x, histtype='step', color=color, linestyle='-', bins=bins, label=label, normed=True, alpha=.5)\n",
    "        else:\n",
    "            ax.hist(x, histtype='step', color=color, linestyle='-', bins=bins, normed=True, alpha=.5)\n",
    "ax.set_xlabel('feature value')\n",
    "ax.set_yticks([])\n",
    "ax.set_ylabel('normalized histogram')\n",
    "ax.set_title('top %d class-conditional features histograms (%d bins)' % (nfeatures, bins))\n",
    "ax.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For completeness, and because the plot would look prettier, I also plotted the two histograms across all features but conditioned on the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "good = np.array(train.X)[train.Y==1, :].ravel()\n",
    "bad  = np.array(train.X)[train.Y==0, :].ravel()\n",
    "goodnz = good[good!=0]\n",
    "badnz  = bad[bad!=0]\n",
    "bins = 64\n",
    "ax.hist(goodnz, bins=bins, label=thumb(1, unicode_symbols=False), histtype='step', color='red')\n",
    "ax.hist(badnz,  bins=bins, label=thumb(0, unicode_symbols=False), histtype='step', color='black')\n",
    "ax.legend(loc='best')\n",
    "ax.set_xlabel('coordinate value')\n",
    "ax.set_ylabel('bin count')\n",
    "ax.set_title('Distribution of coordinate values, marginal over all cases.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, my nonzero feature values seemed to be beta-distributed. Not Gaussian, Bernoulli, or multinomial. Unfortunately, those three were the only options for `sklearn.naive_bayes`. So I tried each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "for BaseClass, name in zip((GaussianNB, BernoulliNB, MultinomialNB),\n",
    "                           ('Gaussian   ', 'Bernoulli  ', 'Multinomial')):\n",
    "    class NaiveBayesClassifier(ClassifierBase, BaseClass): pass\n",
    "    nbClassifier = NaiveBayesClassifier()\n",
    "    nbClassifier.fit()\n",
    "    print name,\n",
    "    nbClassifier.sayErr()\n",
    "    \n",
    "class NaiveBayesMultinomialClassifier(ClassifierBase, MultinomialNB): pass\n",
    "nbClassifier = NaiveBayesMultinomialClassifier()\n",
    "nbClassifier.fit()\n",
    "classifiers.append(nbClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm not sure why the Gaussian classifier actually did the *worst*. Among the options, 'Gaussian' is what I would say is closest to the above beta-like histograms. Regardless, the Naieve Bayes all did worse than the logistic regression classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree\n",
    "\n",
    "Using `sklearn.tree.DecisionTreeClassifier`. I also tried using `nltk`'s `DecisionTreeClassifier`, with pretty bad results. I suspect that I'm just not providing features to `nltk` in the manner it expects correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# scikit learn\n",
    "from sklearn import tree\n",
    "class TreeClassifier(ClassifierBase, tree.DecisionTreeClassifier):\n",
    "    pass \n",
    "mvals = range(1,100, 10)\n",
    "errors = []\n",
    "for m in tqdm_notebook(mvals):\n",
    "    treeClassifier = TreeClassifier(max_depth=m)\n",
    "    treeClassifier.fit()\n",
    "    errors.append(treeClassifier.getErrRate(test.X, test.Y))\n",
    "bestIndex = np.argmin(errors)\n",
    "print 'Best m=%d (error of %.1f%%).' % (mvals[bestIndex], errors[bestIndex]*100)\n",
    "treeClassifier = TreeClassifier(max_depth=mvals[bestIndex])\n",
    "treeClassifier.fit()\n",
    "classifiers.append(treeClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# scikit learn - random forest\n",
    "import sklearn.ensemble\n",
    "import numpy as np\n",
    "class RandomForestClassifier(ClassifierBase, sklearn.ensemble.RandomForestClassifier):\n",
    "    pass \n",
    "errors = []\n",
    "mvals =  range(1, int(np.sqrt(train.X.shape[1])),10)\n",
    "for m in mvals:\n",
    "    randomForestClassifier = RandomForestClassifier(max_depth=m)\n",
    "    randomForestClassifier.fit()\n",
    "    print m,\n",
    "    errors.append(randomForestClassifier.getErrRate(test.X, test.Y))\n",
    "    randomForestClassifier.sayErr()\n",
    "bestIndex = np.argmin(errors)\n",
    "randomForestClassifier = RandomForestClassifier(max_depth=mvals[bestIndex])\n",
    "randomForestClassifier.fit()\n",
    "classifiers.append(randomForestClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Since `sklearn`'s consistent API makes trying different methods so easy,\n",
    "I could readilyi try what the Wikipedia article on Naive Bayes suggested would be a slightly better method--a support-vector machine classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NuSVCClassifier(ClassifierBase, sklearn.svm.NuSVC): pass\n",
    "\n",
    "kernel = 'rbf'\n",
    "nuSVCClassifier = NuSVCClassifier(kernel=kernel, gamma=1./train.X.shape[1])\n",
    "nuSVCClassifier.fit()\n",
    "\n",
    "nuSVCClassifier.sayErr(test=False, datasetName='Train (%s kernel)' % kernel)\n",
    "nuSVCClassifier.sayErr(datasetName='Test  (%s kernel)' % kernel)\n",
    "\n",
    "classifiers.append(nuSVCClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM with a radial basis function kernel provided the best result so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What were the most and least confident cases?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allTexts = list(train.texts)\n",
    "allTexts.extend(test.texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allX = np.vstack([train.X, test.X])\n",
    "allY = np.hstack([train.Y, test.Y])\n",
    "scores      = nuSVCClassifier.decision_function(allX)\n",
    "predictions = nuSVCClassifier.predict(allX)\n",
    "correct = predictions == allY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "order = np.argsort(np.abs(scores))\n",
    "print 'Least confident:'\n",
    "for i in range(10):\n",
    "    j = order[i]\n",
    "    print scores[j], allTexts[j]\n",
    "print\n",
    "print 'Most confident:'\n",
    "for i in range(10):\n",
    "    j = order[-i-1]\n",
    "    print scores[j], allTexts[j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What were the most confident mispredictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cases = 0\n",
    "for i in range(len(predictions)):\n",
    "    if cases > 10:\n",
    "        break\n",
    "    j = order[-i-1]\n",
    "    if not correct[j]:\n",
    "        print scores[j], predictions[j], allY[j], allTexts[j]\n",
    "        cases += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Litotes seems to be not a small problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.cluster\n",
    "clusterer = sklearn.cluster.SpectralClustering(n_clusters=3, gamma=nuSVCClassifier.gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clusterer.fit(train.X)\n",
    "clusterPredictions = clusterer.labels_.astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "j = 0\n",
    "clusterIDs = list(set(clusterPredictions))\n",
    "clusterIDs.sort()\n",
    "clusterMembers = {k:[] for k in clusterIDs}\n",
    "for cluster, dataID in zip(clusterPredictions, range(len(train.texts))):\n",
    "    clusterMembers[cluster].append(dataID)\n",
    "\n",
    "for k in clusterIDs:\n",
    "    print '#### CLUSTER %d ####' % (k+1,)\n",
    "    for i in range(42):\n",
    "        print ' ' + wrap(train.item2str(clusterMembers[k][i]), indent=12)\n",
    "    print '\\n\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only trend I see is that perhaps the reviews in one cluster are a little more terse? Though they're all pretty short. I had hope that maybe these might cluster into movie, restaurant, and product reviews, and then a separate classifier could be generated for each, but I'm not seeing that  with the limited effort I've put forth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature space visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nrows = 8\n",
    "ncols = 8\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=bigFig)\n",
    "\n",
    "skip = 5\n",
    "\n",
    "for i in range(nrows):\n",
    "    for j in range(ncols):\n",
    "        order = np.argsort(prevalences)[::-1]\n",
    "        ax = axes[i,j]\n",
    "        a = order[skip+j]\n",
    "        b = order[skip+i+nrows]\n",
    "        ax.scatter(\n",
    "            train.X[:, a],\n",
    "            train.X[:, b],\n",
    "            c=train.Y,\n",
    "            cmap=plt.get_cmap('RdYlGn'),\n",
    "            s=42,\n",
    "                  )\n",
    "        if i == nrows-1:\n",
    "            ax.set_xlabel(grams[a])\n",
    "        ax.set_xticks([])\n",
    "        if j == 0:\n",
    "            ax.set_ylabel(grams[b])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "fig.subplots_adjust(wspace=0.01, hspace=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probably, since the documents are so short, we're not gaining much from looking for counts of features, TF-IDF normalized or not. Just binary features would likely be almost as informative.\n",
    "\n",
    "The features almost always are not present together, for this small selection of pairings.\n",
    "\n",
    "You can imagine manually constructing a crude decision tree from this plot--some features like 'very', or, bizzarely, 'to' seem to be sufficient, but not necessary, indicators of negative review, and 'of', 'this', 'phone', 'good', 'with', are positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network\n",
    "For fun, I also tried a deep neural network-we'll use the Keras frontend [5] on the Theano GPU backend [4]. Classification performance was comparable to the other good methods, though this would have been much harder to set up if I hadn't already been using this software for a different project. The relatively low training error makes me suspect that some overfitting might be in effect, though generalization to the test set doesn't suffer any more than for the other good methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "import keras.wrappers.scikit_learn\n",
    "class KerasClassifier(ClassifierBase):\n",
    "    \n",
    "    def __init__(self, nlayers=1, layerSize=60,\n",
    "                 compileKwargs=dict(\n",
    "                    loss='binary_crossentropy', optimizer='adam',\n",
    "                    metrics=['accuracy']\n",
    "                 )\n",
    "                ):\n",
    "\n",
    "        def makeModel():\n",
    "            model = Sequential()\n",
    "            model.add(Dense(layerSize, input_dim=train.X.shape[1], init='normal', activation='relu'))\n",
    "            # Add additional layers if requested.\n",
    "            for i in range(nlayers - 1):\n",
    "                model.add(Dense(layerSize, init='normal', activation='relu'))\n",
    "                model.add(Dense(layerSize, init='normal', activation='relu'))\n",
    "            # Add output layer.\n",
    "            model.add(Dense(1, init='normal', activation='sigmoid'))\n",
    "            model.compile(**compileKwargs)\n",
    "            return model\n",
    "        self.makeModel = makeModel\n",
    "        \n",
    "        super(KerasClassifier, self).__init__()\n",
    "        \n",
    "    def displayModel(self, model=None):\n",
    "        if model is None:\n",
    "            model = self.makeModel()\n",
    "        from keras.utils.visualize_util import plot\n",
    "        from IPython.display import Image\n",
    "        p = plot(model, to_file='model.png', show_shapes=True)\n",
    "        return Image(filename='model.png')\n",
    "    \n",
    "    def fit(self, X=None, Y=None, **kwargs):\n",
    "        if X is None: X = self.train.X\n",
    "        if Y is None: Y = self.train.Y\n",
    "        for key, val in dict(nb_epoch=10, batch_size=5, verbose=0).items():\n",
    "            if key not in kwargs:\n",
    "                kwargs[key] = val\n",
    "        self.estimator = keras.wrappers.scikit_learn.KerasClassifier(build_fn=self.makeModel, **kwargs)\n",
    "        self.estimator.fit(X, Y)\n",
    "        \n",
    "    def predict(self, X=None):\n",
    "        if X is None: X = self.test.X\n",
    "        return self.estimator.predict(X, verbose=False).ravel().astype(int)\n",
    "    \n",
    "kerasClassifier = KerasClassifier(nlayers=3, layerSize=64)\n",
    "kerasClassifier.fit(nb_epoch=12, verbose=0)\n",
    "kerasClassifier.sayErr(test=False)\n",
    "kerasClassifier.sayErr()\n",
    "\n",
    "classifiers.append(kerasClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network used was all feedforward and dense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "kerasClassifier.displayModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Refit all the classifiers with the TF-IDF featurization.\n",
    "for classifier in tqdm_notebook(classifiers):\n",
    "    classifier.fit()\n",
    "group = ClassifierGroup(classifiers)\n",
    "rocComp = group.rocComparison()\n",
    "group.summaryTable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "With the experiments done here, a support vector machine on TF-IDF features produced the best generalization to the test data among the methods tried, though a test error of ~17% is still not great. However, I didn't perform a full grid search of classifiers and featurizations, and several of the methods contained parameters whose values I wasn't able to explore fully through a cross-validation strategy. Exploring these parameters of the various featurizations and classifiers presented might produce incremental gains. Additionally, while the neural network classifier presented at the end would normally require extra setup to test, `sklearn` does include multilayer perceptron supervised classifiers at `sklearn.neural_network.MLPClassifier` which would be more portable to machines without GPUs or the CUDA toolkit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography\n",
    "1. *Thumbs up? Sentiment Classification using Machine Learning Techniques*. Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.\n",
    "2. *Scikit-learn: Machine Learning in Python.* Pedregosa et al., JMLR 12, pp. 2825-2830, 2011.\n",
    "3. *Natural Language Processing with Python.* Steven Bird, Edward Loper, and Ewan Klein. O’Reilly Media Inc., 2009.\n",
    "4. *Theano: A Python framework for fast computation of mathematical expressions.* Theano development team. `http://deeplearning.net/software/theano/`, 2016.\n",
    "5. *Keras.* Francois Chollet. `https:/github.com/fchollet/keras`, 2015.\n",
    "6. *Machine Learning: A Probabilistic Perspective.* Kevin Murphy. MIT Press, 2012.\n",
    "7. *Scikit-Learn: Generalized Linear Models*. `http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
